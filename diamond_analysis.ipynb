{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Diamond Dynamics: Price Prediction and Market Segmentation\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Data preprocessing and feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ANN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Pickle for saving models\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# ============================================\n",
    "# 1. DATA LOADING AND INITIAL INSPECTION\n",
    "# ============================================\n",
    "\n",
    "# Load dataset (replace with your actual file path)\n",
    "def load_data(filepath='diamonds.csv'):\n",
    "    \"\"\"Load diamond dataset\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "    return df\n",
    "\n",
    "# ============================================\n",
    "# 2. DATA CLEANING AND PREPROCESSING\n",
    "# ============================================\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and preprocess the data\"\"\"\n",
    "    \n",
    "    # Create copy\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values before cleaning:\")\n",
    "    print(df_clean.isnull().sum())\n",
    "    \n",
    "    # Handle zero/invalid values in x, y, z\n",
    "    for col in ['x', 'y', 'z']:\n",
    "        # Replace zeros with NaN\n",
    "        df_clean[col] = df_clean[col].replace(0, np.nan)\n",
    "        # Replace with median based on carat\n",
    "        df_clean[col] = df_clean.groupby(pd.cut(df_clean['carat'], \n",
    "                                                bins=[0, 0.5, 1, 1.5, 2, 3, 5]))[col].transform(\n",
    "                                                    lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # Drop any remaining rows with NaN (if any)\n",
    "    df_clean = df_clean.dropna()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    \n",
    "    print(f\"\\nShape after cleaning: {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "# ============================================\n",
    "# 3. OUTLIER HANDLING\n",
    "# ============================================\n",
    "\n",
    "def handle_outliers(df):\n",
    "    \"\"\"Handle outliers using IQR method\"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    numerical_cols = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        Q1 = df_out[col].quantile(0.25)\n",
    "        Q3 = df_out[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Cap outliers\n",
    "        df_out[col] = np.where(df_out[col] < lower_bound, lower_bound, df_out[col])\n",
    "        df_out[col] = np.where(df_out[col] > upper_bound, upper_bound, df_out[col])\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# ============================================\n",
    "# 4. SKEWNESS HANDLING\n",
    "# ============================================\n",
    "\n",
    "def handle_skewness(df):\n",
    "    \"\"\"Handle skewness in numerical features\"\"\"\n",
    "    df_skew = df.copy()\n",
    "    \n",
    "    # Check skewness\n",
    "    numerical_cols = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
    "    print(\"Skewness before transformation:\")\n",
    "    for col in numerical_cols:\n",
    "        print(f\"{col}: {df_skew[col].skew():.4f}\")\n",
    "    \n",
    "    # Apply log transformation to highly skewed features\n",
    "    skewed_cols = ['price', 'carat', 'x', 'y', 'z']\n",
    "    for col in skewed_cols:\n",
    "        if col in df_skew.columns:\n",
    "            df_skew[f'{col}_log'] = np.log1p(df_skew[col])\n",
    "    \n",
    "    print(\"\\nSkewness after log transformation (price_log):\", \n",
    "          df_skew['price_log'].skew())\n",
    "    \n",
    "    return df_skew\n",
    "\n",
    "# ============================================\n",
    "# 5. EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"Perform comprehensive EDA\"\"\"\n",
    "    \n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # 1. Distribution plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    numerical_cols = ['carat', 'price', 'x', 'y', 'z', 'depth']\n",
    "    for idx, col in enumerate(numerical_cols[:6]):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[idx])\n",
    "        axes[idx].set_title(f'Distribution of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Count plots for categorical features\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    categorical_cols = ['cut', 'color', 'clarity']\n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        order = df[col].value_counts().index\n",
    "        sns.countplot(data=df, x=col, ax=axes[idx], order=order)\n",
    "        axes[idx].set_title(f'Distribution of {col}')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('categorical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Price variation with categorical features\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        sns.boxplot(data=df, x=col, y='price', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Price by {col}')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('price_vs_categorical.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Correlation heatmap\n",
    "    numerical_df = df.select_dtypes(include=[np.number])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr_matrix = numerical_df.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                fmt='.2f', square=True)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Scatterplot matrix\n",
    "    sns.pairplot(df[['carat', 'price', 'x', 'y', 'z']], \n",
    "                diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "    plt.suptitle('Pairwise Relationships', y=1.02)\n",
    "    plt.savefig('pairplot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Carat vs Price\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.regplot(data=df, x='carat', y='price', scatter_kws={'alpha': 0.3}, \n",
    "               line_kws={'color': 'red'})\n",
    "    plt.title('Carat vs Price (with Regression Line)')\n",
    "    plt.xlabel('Carat')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.savefig('carat_vs_price.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Average price per category\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        avg_price = df.groupby(col)['price'].mean().sort_values()\n",
    "        avg_price.plot(kind='bar', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Average Price by {col}')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Average Price (USD)')\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('avg_price_categories.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 6. FEATURE ENGINEERING\n",
    "# ============================================\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Create new features\"\"\"\n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # Convert price to INR (assuming 1 USD = 83 INR)\n",
    "    conversion_rate = 83\n",
    "    df_fe['price_inr'] = df_fe['price'] * conversion_rate\n",
    "    \n",
    "    # New engineered features\n",
    "    df_fe['volume'] = df_fe['x'] * df_fe['y'] * df_fe['z']\n",
    "    df_fe['price_per_carat'] = df_fe['price'] / df_fe['carat']\n",
    "    df_fe['dimension_ratio'] = (df_fe['x'] + df_fe['y']) / (2 * df_fe['z'])\n",
    "    \n",
    "    # Carat categories\n",
    "    conditions = [\n",
    "        df_fe['carat'] < 0.5,\n",
    "        (df_fe['carat'] >= 0.5) & (df_fe['carat'] <= 1.5),\n",
    "        df_fe['carat'] > 1.5\n",
    "    ]\n",
    "    choices = ['Light', 'Medium', 'Heavy']\n",
    "    df_fe['carat_category'] = np.select(conditions, choices, default='Medium')\n",
    "    \n",
    "    # Additional features\n",
    "    df_fe['surface_area'] = 2 * (df_fe['x']*df_fe['y'] + df_fe['x']*df_fe['z'] + df_fe['y']*df_fe['z'])\n",
    "    df_fe['density'] = df_fe['carat'] / df_fe['volume']\n",
    "    \n",
    "    # Interaction features\n",
    "    df_fe['carat_cut_interaction'] = df_fe['carat'] * pd.Categorical(df_fe['cut']).codes\n",
    "    df_fe['carat_color_interaction'] = df_fe['carat'] * pd.Categorical(df_fe['color']).codes\n",
    "    \n",
    "    print(\"New features created:\")\n",
    "    print(df_fe[['volume', 'price_per_carat', 'dimension_ratio', \n",
    "                'carat_category', 'price_inr']].head())\n",
    "    \n",
    "    return df_fe\n",
    "\n",
    "# ============================================\n",
    "# 7. FEATURE SELECTION\n",
    "# ============================================\n",
    "\n",
    "def select_features(df, target_col='price'):\n",
    "    \"\"\"Select most important features\"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col, 'price_inr', 'price_log'], errors='ignore')\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    X_encoded = pd.get_dummies(X, columns=['cut', 'color', 'clarity', 'carat_category'], \n",
    "                              drop_first=True)\n",
    "    \n",
    "    # Method 1: Random Forest Feature Importance\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_encoded, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_encoded.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Features by Importance:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Method 2: Correlation with target\n",
    "    X_encoded['target'] = y\n",
    "    correlation = X_encoded.corr()['target'].abs().sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 Features by Correlation:\")\n",
    "    print(correlation[1:11])\n",
    "    \n",
    "    # Select top features (example: top 15)\n",
    "    top_features = feature_importance.head(15)['feature'].tolist()\n",
    "    \n",
    "    return top_features, X_encoded[top_features]\n",
    "\n",
    "# ============================================\n",
    "# 8. ENCODING AND SCALING\n",
    "# ============================================\n",
    "\n",
    "def prepare_data(df, features, target_col='price'):\n",
    "    \"\"\"Prepare data for modeling\"\"\"\n",
    "    \n",
    "    # Select features\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_cols),\n",
    "            ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols)\n",
    "        ])\n",
    "    \n",
    "    return X, y, preprocessor\n",
    "\n",
    "# ============================================\n",
    "# 9. REGRESSION MODELING\n",
    "# ============================================\n",
    "\n",
    "def build_regression_models(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"Build and compare regression models\"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=0.1),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', model)\n",
    "        ])\n",
    "        \n",
    "        # Train model\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = pipeline.predict(X_train)\n",
    "        y_pred_test = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Train MAE': train_mae,\n",
    "            'Test MAE': test_mae,\n",
    "            'Train RMSE': train_rmse,\n",
    "            'Test RMSE': test_rmse,\n",
    "            'Train R²': train_r2,\n",
    "            'Test R²': test_r2\n",
    "        })\n",
    "        \n",
    "        # Save the best model (based on test R²)\n",
    "        if name == 'Random Forest':  # Example: save Random Forest\n",
    "            joblib.dump(pipeline, 'best_regression_model.pkl')\n",
    "            print(f\"\\nSaved {name} model as 'best_regression_model.pkl'\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nRegression Models Performance:\")\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ============================================\n",
    "# 10. ARTIFICIAL NEURAL NETWORK (ANN)\n",
    "# ============================================\n",
    "\n",
    "def build_ann_model(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    \"\"\"Build ANN model\"\"\"\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Define ANN architecture\n",
    "    model = Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train_processed.shape[1],)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_processed, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_pred = model.predict(X_train_processed, verbose=0)\n",
    "    test_pred = model.predict(X_test_processed, verbose=0)\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    \n",
    "    ann_results = pd.DataFrame([{\n",
    "        'Model': 'ANN',\n",
    "        'Train MAE': train_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train R²': train_r2,\n",
    "        'Test R²': test_r2\n",
    "    }])\n",
    "    \n",
    "    print(\"\\nANN Performance:\")\n",
    "    print(ann_results.to_string())\n",
    "    \n",
    "    # Save ANN model\n",
    "    model.save('ann_diamond_model.h5')\n",
    "    print(\"\\nSaved ANN model as 'ann_diamond_model.h5'\")\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0].set_title('Model Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(history.history['mae'], label='Training MAE')\n",
    "    axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "    axes[1].set_title('Model MAE')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ann_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return ann_results, model\n",
    "\n",
    "# ============================================\n",
    "# 11. CLUSTERING FOR MARKET SEGMENTATION\n",
    "# ============================================\n",
    "\n",
    "def perform_clustering(df):\n",
    "    \"\"\"Perform K-Means clustering for market segmentation\"\"\"\n",
    "    \n",
    "    # Prepare data for clustering (exclude price and target variables)\n",
    "    clustering_features = ['carat', 'x', 'y', 'z', 'depth', 'table']\n",
    "    categorical_features = ['cut', 'color', 'clarity']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df_cluster = df.copy()\n",
    "    le_cut = LabelEncoder()\n",
    "    le_color = LabelEncoder()\n",
    "    le_clarity = LabelEncoder()\n",
    "    \n",
    "    df_cluster['cut_encoded'] = le_cut.fit_transform(df_cluster['cut'])\n",
    "    df_cluster['color_encoded'] = le_color.fit_transform(df_cluster['color'])\n",
    "    df_cluster['clarity_encoded'] = le_clarity.fit_transform(df_cluster['clarity'])\n",
    "    \n",
    "    # Combine all features for clustering\n",
    "    cluster_cols = clustering_features + ['cut_encoded', 'color_encoded', 'clarity_encoded']\n",
    "    X_cluster = df_cluster[cluster_cols]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_cluster)\n",
    "    \n",
    "    # Determine optimal number of clusters using Elbow method\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, 11)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        if k > 1:  # Silhouette score requires at least 2 clusters\n",
    "            silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Plot Elbow method and Silhouette scores\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(K_range, inertia, marker='o')\n",
    "    axes[0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0].set_ylabel('Inertia')\n",
    "    axes[0].set_title('Elbow Method for Optimal k')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    axes[1].plot(list(K_range)[1:], silhouette_scores, marker='o', color='orange')\n",
    "    axes[1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].set_title('Silhouette Scores')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose optimal k (example: k=4)\n",
    "    optimal_k = 4\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    df_cluster['cluster'] = clusters\n",
    "    \n",
    "    # Save the clustering model\n",
    "    joblib.dump(kmeans, 'kmeans_clustering_model.pkl')\n",
    "    joblib.dump(scaler, 'clustering_scaler.pkl')\n",
    "    \n",
    "    # PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    df_cluster['pca1'] = X_pca[:, 0]\n",
    "    df_cluster['pca2'] = X_pca[:, 1]\n",
    "    \n",
    "    # Visualize clusters\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(df_cluster['pca1'], df_cluster['pca2'], \n",
    "                         c=df_cluster['cluster'], cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title('Diamond Clusters (PCA Visualization)')\n",
    "    plt.savefig('clusters_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_analysis = df_cluster.groupby('cluster').agg({\n",
    "        'price': ['mean', 'median', 'count'],\n",
    "        'carat': ['mean', 'median'],\n",
    "        'cut': lambda x: x.mode()[0],\n",
    "        'color': lambda x: x.mode()[0],\n",
    "        'clarity': lambda x: x.mode()[0]\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nCluster Analysis:\")\n",
    "    print(cluster_analysis)\n",
    "    \n",
    "    # Create cluster names based on characteristics\n",
    "    cluster_names = {}\n",
    "    for cluster_num in range(optimal_k):\n",
    "        cluster_data = df_cluster[df_cluster['cluster'] == cluster_num]\n",
    "        \n",
    "        avg_price = cluster_data['price'].mean()\n",
    "        avg_carat = cluster_data['carat'].mean()\n",
    "        common_cut = cluster_data['cut'].mode()[0]\n",
    "        \n",
    "        if avg_price > df_cluster['price'].quantile(0.75) and avg_carat > 1.0:\n",
    "            name = \"Premium Heavy Diamonds\"\n",
    "        elif avg_price < df_cluster['price'].quantile(0.25) and avg_carat < 0.5:\n",
    "            name = \"Affordable Small Diamonds\"\n",
    "        elif avg_price > df_cluster['price'].quantile(0.5) and common_cut in ['Premium', 'Ideal']:\n",
    "            name = \"High-Quality Premium Diamonds\"\n",
    "        else:\n",
    "            name = \"Mid-range Balanced Diamonds\"\n",
    "        \n",
    "        cluster_names[cluster_num] = name\n",
    "    \n",
    "    print(\"\\nCluster Names:\")\n",
    "    for cluster_num, name in cluster_names.items():\n",
    "        print(f\"Cluster {cluster_num}: {name}\")\n",
    "    \n",
    "    # Save cluster names\n",
    "    joblib.dump(cluster_names, 'cluster_names.pkl')\n",
    "    \n",
    "    return df_cluster, cluster_names, kmeans, scaler\n",
    "\n",
    "# ============================================\n",
    "# 12. MAIN EXECUTION PIPELINE\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DIAMOND DYNAMICS: Price Prediction & Market Segmentation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"\\n1. LOADING DATA...\")\n",
    "    df = load_data()\n",
    "    \n",
    "    # Step 2: Data cleaning\n",
    "    print(\"\\n2. DATA CLEANING...\")\n",
    "    df_clean = clean_data(df)\n",
    "    \n",
    "    # Step 3: Handle outliers\n",
    "    print(\"\\n3. HANDLING OUTLIERS...\")\n",
    "    df_outliers = handle_outliers(df_clean)\n",
    "    \n",
    "    # Step 4: Handle skewness\n",
    "    print(\"\\n4. HANDLING SKEWNESS...\")\n",
    "    df_skew = handle_skewness(df_outliers)\n",
    "    \n",
    "    # Step 5: Perform EDA\n",
    "    print(\"\\n5. EXPLORATORY DATA ANALYSIS...\")\n",
    "    perform_eda(df_skew)\n",
    "    \n",
    "    # Step 6: Feature engineering\n",
    "    print(\"\\n6. FEATURE ENGINEERING...\")\n",
    "    df_fe = feature_engineering(df_skew)\n",
    "    \n",
    "    # Step 7: Feature selection\n",
    "    print(\"\\n7. FEATURE SELECTION...\")\n",
    "    top_features, X_selected = select_features(df_fe)\n",
    "    \n",
    "    # Step 8: Prepare data for modeling\n",
    "    print(\"\\n8. PREPARING DATA FOR MODELING...\")\n",
    "    X, y, preprocessor = prepare_data(df_fe, top_features + ['cut', 'color', 'clarity', 'carat_category'])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Step 9: Build regression models\n",
    "    print(\"\\n9. BUILDING REGRESSION MODELS...\")\n",
    "    regression_results = build_regression_models(\n",
    "        X_train, X_test, y_train, y_test, preprocessor\n",
    "    )\n",
    "    \n",
    "    # Step 10: Build ANN model\n",
    "    print(\"\\n10. BUILDING ANN MODEL...\")\n",
    "    ann_results, ann_model = build_ann_model(\n",
    "        X_train, X_test, y_train, y_test, preprocessor\n",
    "    )\n",
    "    \n",
    "    # Step 11: Perform clustering\n",
    "    print(\"\\n11. PERFORMING CLUSTERING...\")\n",
    "    df_clustered, cluster_names, kmeans_model, cluster_scaler = perform_clustering(df_fe)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nModels saved:\")\n",
    "    print(\"1. Best Regression Model: best_regression_model.pkl\")\n",
    "    print(\"2. ANN Model: ann_diamond_model.h5\")\n",
    "    print(\"3. Clustering Model: kmeans_clustering_model.pkl\")\n",
    "    print(\"4. Cluster Names: cluster_names.pkl\")\n",
    "    print(\"5. Clustering Scaler: clustering_scaler.pkl\")\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
